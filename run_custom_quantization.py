import os
import torch
from typing import Optional, List
import shutil

# --- 1. SDK ë° ì»¤ìŠ¤í…€ ë¡œì§ì— í•„ìš”í•œ ëª¨ë“  í´ë˜ìŠ¤ ì„í¬íŠ¸ ---
try:
    from furiosa_llm.optimum import QuantizerForCausalLM, QuantizationConfig
    from furiosa_llm.optimum.dataset_utils import create_data_loader
    from furiosa_llm_models.llama3.symbolic.mlperf_submission import LlamaForCausalLM as FuriosaLlamaForCausalLM
except ImportError:
    print("ì˜¤ë¥˜: furiosa-llm SDKê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜, í•„ìš”í•œ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()


# --- 2. ê¸ˆì§€í•  í† í° ID ë¡œë“œ ---
banned_tokens_file = "banned_tokens.txt"
with open(banned_tokens_file, 'r', encoding='utf-8') as f:
    banned_tokens_str = f.read().replace('[', '').replace(']', '')
    banned_token_ids = [int(x.strip()) for x in banned_tokens_str.split(',')]

print(f"ì´ {len(banned_token_ids)}ê°œì˜ í† í°ì„ ê¸ˆì§€í•˜ë„ë¡ ì„¤ì •ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.")


# --- 3. â­ï¸â­ï¸â­ï¸ SDK í´ë˜ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì • (ëª½í‚¤ íŒ¨ì¹­) â­ï¸â­ï¸â­ï¸ ---
# Quantizerê°€ ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ì „ì—, SDKì˜ ì›ë³¸ í´ë˜ìŠ¤ ìì²´ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.

# 3-1. SDKì˜ ì›ë³¸ forward ë©”ì„œë“œë¥¼ ë°±ì—…í•´ ë‘¡ë‹ˆë‹¤.
original_forward = FuriosaLlamaForCausalLM.forward
print("SDKì˜ ì›ë³¸ LlamaForCausalLM.forward ë©”ì„œë“œë¥¼ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")

# 3-2. ìš°ë¦¬ê°€ ì£¼ì…í•  ìƒˆë¡œìš´ forward í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
#      ì‹œê·¸ë‹ˆì²˜ëŠ” ì›ë³¸ íŒŒì¼ê³¼ ì™„ë²½í•˜ê²Œ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
def patched_forward(
    self,
    input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None,
    causal_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None,
    new_key_location: Optional[torch.IntTensor] = None, new_value_location: Optional[torch.IntTensor] = None,
    past_valid_key_indices: Optional[torch.IntTensor] = None, past_valid_value_indices: Optional[torch.IntTensor] = None,
    is_prefill: bool = True, bucket_size: int = 2048, labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, **kwargs
):
    # ë°±ì—…í•´ ë‘” ì›ë³¸ forward ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì—¬ SDKì˜ ëª¨ë“  ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    outputs = original_forward(
        self, input_ids=input_ids, attention_mask=attention_mask, causal_mask=causal_mask,
        position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds,
        new_key_location=new_key_location, new_value_location=new_value_location,
        past_valid_key_indices=past_valid_key_indices, past_valid_value_indices=past_valid_value_indices,
        is_prefill=is_prefill, bucket_size=bucket_size, labels=labels, use_cache=use_cache,
        output_attentions=output_attentions, output_hidden_states=output_hidden_states,
        return_dict=return_dict, **kwargs
    )
    
    # ì›ë³¸ ë©”ì„œë“œê°€ ëë‚œ í›„, ìš°ë¦¬ì˜ ì»¤ìŠ¤í…€ ë¡œì§ì„ ê²°ê³¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    logits = outputs.logits
    if banned_token_ids:
        logits[:, :, banned_token_ids] = torch.finfo(logits.dtype).min
    
    # ìˆ˜ì •ëœ ê²°ê³¼ë¥¼ ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    outputs.logits = logits
    return outputs

# 3-3. SDKì˜ Llama í´ë˜ìŠ¤ì˜ forward ë©”ì„œë“œë¥¼ ìš°ë¦¬ê°€ ë§Œë“  í•¨ìˆ˜ë¡œ êµì²´í•©ë‹ˆë‹¤.
FuriosaLlamaForCausalLM.forward = patched_forward
print("SDKì˜ LlamaForCausalLM í´ë˜ìŠ¤ì— ì»¤ìŠ¤í…€ ë¡œì§ì„ ì„±ê³µì ìœ¼ë¡œ ì£¼ì…(ëª½í‚¤ íŒ¨ì¹­)í–ˆìŠµë‹ˆë‹¤.")


# --- 4. ë©”ì¸ ì–‘ìí™” ì‹¤í–‰ ë¡œì§ ---
def main():
    # ì´ì œ ìˆœì • ëª¨ë¸ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    model_id = "meta-llama/Llama-3.3-70B-Instruct"
    
    # ì–‘ìí™”ëœ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ
    quantized_model_output_dir = "./quantized_not_CJ_llama_for_npu"

    # â­ï¸ ìºì‹œë¥¼ ì§€ì›Œ ì´ì „ì˜ ì‹¤íŒ¨í•œ ì‹œë„ì˜ ì˜í–¥ì„ ì™„ì „íˆ ì œê±°í•©ë‹ˆë‹¤.
    cache_dir = os.path.expanduser("~/.cache/furiosa/llm")
    if os.path.exists(cache_dir):
        print(f"'{cache_dir}' ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
        shutil.rmtree(cache_dir)

    print(f"\n'{model_id}' ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì–‘ìí™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    dataloader = create_data_loader(
        tokenizer=model_id,
        dataset_name_or_path="mit-han-lab/pile-val-backup",
        dataset_split="validation",
        num_samples=5,
        max_sample_length=2048, # SDK ê¸°ë³¸ ë²„í‚· í¬ê¸°ì— ë§ì¶¤
    )

    quantizer = QuantizerForCausalLM.from_pretrained(model_id)
    
    quantizer.quantize(
        quantized_model_output_dir,
        dataloader,
        QuantizationConfig.w_f8_a_f8_kv_f8()
    )

    print(f"\nğŸ‰ ì–‘ìí™” ì„±ê³µ! ê²°ê³¼ê°€ '{quantized_model_output_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì´ì œ ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ArtifactBuilderë¡œ ìµœì¢… ì•„í‹°íŒ©íŠ¸ë¥¼ ë¹Œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()
